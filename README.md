ğŸ¤– RAG-based Q&A System with Gemini API
This project implements a Retrieval-Augmented Generation (RAG) pipeline that:
âœ… Splits input text into chunks,
âœ… Stores & indexes them in a ChromaDB vector database,
âœ… Retrieves relevant chunks based on a user query using Sentence Transformers,
âœ… Feeds the context into Google Gemini API to generate an intelligent answer.

âœ¨ Features
âœ… Chunking: Splits long input text into smaller pieces.

âœ… Vector Search: Stores chunks in ChromaDB with embeddings (all-MiniLM-L6-v2).

âœ… Semantic Retrieval: Retrieves topâ€‘K relevant chunks for each query.

âœ… Generative AI: Uses Google Gemini to produce contextâ€‘aware answers.

âœ… Interactive CLI: Ask questions in a loop until you type exit.

ğŸ—ï¸ Tech Stack
Python 3.9+

Sentence Transformers (all-MiniLM-L6-v2 model)

ChromaDB

Google Generative AI (Gemini)

ğŸ“‚ Project Structure
bash
Copy
Edit
ğŸ“¦ rag-gemini-chroma
 â”£ ğŸ“œ main.py            # Main RAG implementation
 â”£ ğŸ“œ requirements.txt   # Python dependencies
 â”£ ğŸ“‚ .chromadb/         # Local ChromaDB storage
 â”— ğŸ“œ README.md
âš™ï¸ Installation
1ï¸âƒ£ Clone the repo:

bash
Copy
Edit
git clone https://github.com/yourusername/rag-gemini-chroma.git
cd rag-gemini-chroma
2ï¸âƒ£ Install dependencies:

bash
Copy
Edit
pip install -r requirements.txt
3ï¸âƒ£ Set up Gemini API Key:
Replace the placeholder in main.py with your API key:

python
Copy
Edit
genai.configure(api_key="YOUR_GEMINI_API_KEY")
â–¶ï¸ Usage
Run the script:

bash
Copy
Edit
python main.py
You will see:

pgsql
Copy
Edit
âœ… Loaded X chunks from input text
Ask a question (or type 'exit'):
ğŸ‘‰ Type a question and press Enter.
ğŸ‘‰ The system retrieves relevant context and Gemini generates an answer.
ğŸ‘‰ Type exit to quit.

ğŸ“Œ Example
Question:
What is PARA on Nasdaq?

Answer:
(Gemini will generate an answer based on retrieved context)

ğŸ“Š How It Works
Pipeline:

chunk_text() â†’ splits input text

create_chroma_collection() â†’ embeds and stores chunks

retrieve_relevant_chunks() â†’ retrieves top-k chunks for a query

generate_answer_gemini() â†’ feeds context and query to Gemini for a final answer

ğŸš€ Future Enhancements
ğŸ“Œ Add support for file uploads (PDF/TXT).

ğŸ“Œ Build a web interface (React/Flask) instead of CLI.

ğŸ“Œ Implement caching for faster repeated queries.

ğŸ¤ Contributing
Feel free to fork this repository and submit PRs for improvements.
